{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18128ee9",
   "metadata": {},
   "source": [
    "# bert-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(PretrainedConfig):\n",
    "            def __init__(\n",
    "            self,\n",
    "                \n",
    "                # bert에 포함된 voca 크키 / 고유 토큰 갯수 / 기본값: 30522\n",
    "            vocab_size=30522,\n",
    "                \n",
    "                #encoder와 pooler층 차원 수 / 기본값: 768\n",
    "            hidden_size=768, \n",
    "                \n",
    "                # encoder hidden layer 수 / 기본값: 12\n",
    "            num_hidden_layers=12, \n",
    "                \n",
    "                #encoder가 가지는 attention head 수 / 기본값: 12\n",
    "            num_attention_heads=12, \n",
    "                \n",
    "                #encoder의 intermediate 차원수 (feed-forward) / 기본값: 3072\n",
    "            intermediate_size=3072, \n",
    "                \n",
    "                # encoder와 pooler의 활성화 함수 기본값: gelu\n",
    "            hidden_act=\"gelu\", \n",
    "                \n",
    "                # embedding과 encoder와 pooler의 fully-connected-layer의 dropout 비율 / 기본값: 0.1\n",
    "            hidden_dropout_prob=0.1, \n",
    "                \n",
    "                 #attention probabilities의 dropout 비율 / 기본값: 0.1\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "                \n",
    "                # 모델이 처리할 수 있는 sequence의 최대 길이 / 기본값: 512\n",
    "            max_position_embeddings=512,\n",
    "                \n",
    "                # token_type_ids의 voca 크기 / 기본값: 2\n",
    "            type_vocab_size=2, \n",
    "                \n",
    "                # 모든 가중치 벡터 초기화에 쓰이는 표준 편차 값/ 기본값: 0.02\n",
    "            initializer_range=0.02, \n",
    "                \n",
    "                # layer normalization layers에 쓰는 epsilon 값/ 기본값: 1e-12 \n",
    "            layer_norm_eps=1e-12, \n",
    "            pad_token_id=0, \n",
    "                \n",
    "                # position embedding의 유형 ('absolute', 'relative_key', 'relative_key_query') / 기본값: 'absolute'\n",
    "            position_embedding_type=\"absolute\", \n",
    "                \n",
    "                # 모델이 마지막 key/value attention들을 반환할 것인가 여부(is_decoder = True 일 때만 의미 있음.)\n",
    "            use_cache=True, \n",
    "                \n",
    "                # classification head의 dropout 비율\n",
    "            classifier_dropout=None, \n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a240f",
   "metadata": {},
   "source": [
    "# Bert-Tokenizer\n",
    "\n",
    "**tokenizer 정의**         \n",
    "    : 주어진 corpus에서 토큰이라 불리는 단위로 나누는 작업임.\n",
    "    \n",
    "  - berttokenizer의 특별한 점.\n",
    "      - wordpiece tokenizer(BPE의 변형 알고리즘) 적용\n",
    "      - BPE(Byte Pair Encoding): OOV(OUT-OF-VOCA) 문제를 완화하기위한 대표적인 서브워드 분리 알고리즘\n",
    "      - 서브워드 분리(subword segmenation); 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들(workplace = work + place)조합으로 구성된 경우가 많음. 이같은 하나의 단어를 서브워드로 분리해 인코딩 & 임베딩하기 위한 전처리 작업.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   ## Tokenizer의 input과 output\n",
    "   \n",
    "   - input\n",
    "       텍스트 (List[str]), 리스트 형태로 반환\n",
    "       \n",
    "       - [\"단어 단어\", \"단어 단어 단어:]\n",
    "       \n",
    "       \n",
    "   - output\n",
    "       토큰화 결과 (Dict), 딕셔너리 형태로 변환\n",
    "       \n",
    "       - {'input_ids': tensor([[101, 343, 3423, 102], [101, 2324, 34245, 343, 102]])\n",
    "         , 'token_type_ids': tensor([[0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
    "         , 'attention_mask': tensor([[1, 1, 1, 1], [1, 1, 1, 1, 1]])}\n",
    "         \n",
    "         \n",
    "         \n",
    "  ## output 들어내기\n",
    "       \n",
    "   ### input_ids\n",
    "   - 각 토큰에 대한 정수 인코딩 결과물.\n",
    "    \n",
    "   - 문장에 존재하는 입력 시퀀스(단어)의 인덱스, bertTokenizer를 이용해 지수를 부여 \n",
    "    \n",
    "   \n",
    "   **예시문장**\n",
    "   \n",
    "   - \"오늘은 꽈배기를 먹었다\"\n",
    "   \n",
    "   - [tokenize]\n",
    "           \n",
    "   - \"CLS\", \"오늘은\", \"꽈배기를\", \"먹었다\", \"SEP\"\n",
    "   \n",
    "   - [encode]\n",
    "           \n",
    "   - 101, 2344, 423534, 2342, 102\n",
    "   \n",
    "   - [decode]\n",
    "   \n",
    "   - \"CLS\", \"오늘은\", \"꽈배기를\", \"먹었다\", \"SEP\"\n",
    "   \n",
    "   \n",
    "   \n",
    "   ### token_type_ids (= segment_ids)\n",
    "   \n",
    "   - pre-training 단계의 NSP(Next Sentence Prediction) 작업을 위해 존재한다.\n",
    "     fine-tuning 시에는 모두 0이다.\n",
    "     \n",
    "     토큰 인덱스를 세그먼트 형태로 변형하여 입력의 첫번째 및 두번째 부분을 나타냄.\n",
    "     이때 인덱스는 [0, 1]에서 선택\n",
    "    \n",
    "     0 = 문장 A 토큰, 1 = 문장 B 토큰\n",
    "    \n",
    "    \n",
    "    \n",
    "   ### attention mask\n",
    "   \n",
    "   - attention을 받지 않도록 하는 부분 구분.\n",
    "    \n",
    "     BERT의 Inference시에는 zero padding으로 입력된 토큰에 대해선 attention score를 받지 못하도록 masking 처리한다.\n",
    "    \n",
    "    \n",
    "     CLS, SEP도 포함해 최대길이를 지정하고 다른 문장에 토큰이 없다면 0으로 masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0541f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
